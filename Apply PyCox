import numpy as np
from numpy import argmax
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import time
from datetime import timedelta
from datetime import datetime
import random
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn_pandas import DataFrameMapper

import torch
import torchtuples as tt

from pycox.models import CoxCC
from pycox.evaluation import EvalSurv


# Transform data
cols_leave = [Dummied Data]
cols_standardize = [Numerical Feature]

standardize = [([col], StandardScaler()) for col in cols_standardize]
leave = [(col, None) for col in cols_leave]
x_mapper = DataFrameMapper(standardize + leave)

x_train = x_mapper.fit_transform(df_train).astype('float32')
x_val = x_mapper.fit_transform(df_val).astype('float32')
x_test = x_mapper.fit_transform(df_test).astype('float32')

get_target = lambda df: (df[Time].values, df[Event].values)
y_train = get_target(df_train)
y_val = get_target(df_val)
durations_test, events_test = get_target(df_test)
val = tt.tuplefy(x_val, y_val)

# Create a function to run neural net with given inputs
def nnet(df,node_str, drop, batchsize, learn):
   
    #Transform the dataset
    df_train = df.copy()
    df_test = df_train.sample(frac=0.2)
    df_train = df_train.drop(df_test.index)
    df_val = df_train.sample(frac=0.2)
    df_train = df_train.drop(df_val.index)
    
    x_train = df_train.drop(columns=[Time,Event]).values.astype('float32')
    x_val = df_val.drop(columns=[Time,Event]).values.astype('float32')
    x_test = df_test.drop(columns=[Time,Event]).values.astype('float32')
    
    get_target = lambda df: (df[Time].values, df[Event].values)
    y_train = get_target(df_train)
    y_val = get_target(df_val)
    durations_test, events_test = get_target(df_test)
    val = tt.tuplefy(x_val, y_val)
    
    in_features = x_train.shape[1]
    num_nodes = node_str
    out_features = 1
    batch_norm = True
    dropout = drop
    output_bias = False
    batch_size = batchsize
    epochs = 512 
    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,
                                  dropout, output_bias=output_bias)
    
    #Train the model
    model = CoxCC(net, tt.optim.Adam)                                #optimizer = tt.optim.Adam
    model.optimizer.set_lr(learn)
    callbacks = [tt.callbacks.EarlyStopping()]
    verbose = False

    log = model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose, val_data=val.repeat(10).cat())
    
    a = model.partial_log_likelihood(*val).mean()                     #logloss
    
    #Test Dataset Performance
    _ = model.compute_baseline_hazards()
    surv = model.predict_surv_df(x_test)
    
    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')
    b =  ev.concordance_td()                                            #Concordance Index
    
    time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)
    
    c =  ev.integrated_brier_score(time_grid)                           #IBS
    
    d = ev.integrated_nbll(time_grid)                                   #INbll

    return a, b, c, d
 
#Array of node structures
all_nodes =np.array([[32],[64],[128],[256],     
            [32,32],[64,64],[128,128],[256,256],[32,64],[64,128],[128,256],     
            [32,32,32],[64,64,64],[128,128,128],[256,256, 256],[32,64,64],[64,128,128],[128,256,256],  
            [32,32,64],[64,64,128],[128,128,256],     
            [32,32,32,32],[64,64,64,64],[128,128,128,128],[256,256,256,256],    
            [32,32,64,64],[64,64,128,128],[128,128,256,256],  
            ])

all_dropouts = [0.1, 0.2]
all_batchsize = [64, 128, 256]
all_lr = [0.001,1e-4,1e-6]

*** Note: Unable to loop optimizer. Might need to run separately***

** Fit the model with loop, export the results to csv file **
n_iter = 200 #Number of random iterations to run 
out = pd.DataFrame(columns=['Nodes','Dropout','Batchsize', 'LearnRate',
                           'Logloss','CIndex','IBS','INbll'])
count=0 
start = time.time()
for x in tqdm(range(n_iter)):
    i = random.randint(0,27) #Select Nodes
    j = random.randint(0,1) #Select Dropout
    k = random.randint(0,2) #Select Batchsize
    m = random.randint(0,2) #Select Learning Rate
    a, b, c, d = nnet(df_modelNPL_dummy,all_nodes[i], all_dropouts[j], all_batchsize[k], all_lr[m])
    
    if b <= 0.716:              # Threshold from previous experiment
        continue
    
    out.loc[len(out)] = (all_nodes[i], all_dropouts[j]
                      , all_batchsize[k], all_lr[m],a,b,c,d)
    count += 1
    elapsed = time.time()-start
    print('Completed {} out of {} iterations in time {} '.format(count,n_iter,
                                         str(timedelta(seconds=elapsed)) ))
    print(out.iloc[[-1]])
   
